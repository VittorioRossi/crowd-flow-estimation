{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fe218b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\hp\\\\Desktop\\\\Master - AI\\\\Semestre II\\\\Computer Vision\\\\CV-crowd-flow-estimation-\\\\notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add project root to sys.path (one directory up from the notebook)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from models.resnet50_backbone import ResNet50Backbone\n",
    "from models.vgg_backbone import VGG19BNBackbone\n",
    "\n",
    "from src.data_loader import ShanghaiTechDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c753fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet50Backbone()\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/resnet50_finetuned.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[0;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m      8\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1541\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1541\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(\n\u001b[0;32m   1542\u001b[0m             opened_file,\n\u001b[0;32m   1543\u001b[0m             map_location,\n\u001b[0;32m   1544\u001b[0m             _weights_only_unpickler,\n\u001b[0;32m   1545\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1546\u001b[0m         )\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1797\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m         f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1797\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m   1799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:573\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, encoding)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(file, \u001b[38;5;241m*\u001b[39m, encoding: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASCII\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Unpickler(file, encoding\u001b[38;5;241m=\u001b[39mencoding)\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:325\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m key \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# Risky operators\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====== Part A evaluation ======\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# common transform + dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "val_dataset_A = ShanghaiTechDataset(\"../data/ShanghaiTech\", part=\"part_A\", split=\"test_data\", transform=transform)\n",
    "val_loader_A = DataLoader(val_dataset_A, batch_size=1)\n",
    "\n",
    "# evaluate each model\n",
    "for name, Cls in [(\"resnet50\", ResNet50Backbone),\n",
    "                  (\"vgg19_bn\", VGG19BNBackbone),]:\n",
    "    # load\n",
    "    model = Cls().to(device)\n",
    "    model.load_state_dict(torch.load(f\"../models/part_A_{name}_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    mae = mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, count_map in val_loader_A:\n",
    "            img = img.to(device)\n",
    "            count = count_map.sum(dim=(1,2)).unsqueeze(1).float().to(device)\n",
    "            out = model(img)\n",
    "            mae += torch.abs(out - count).item()\n",
    "            mse += ((out - count)**2).item()\n",
    "\n",
    "    mae /= len(val_dataset_A)\n",
    "    rmse = (mse / len(val_dataset_A))**0.5\n",
    "    print(f\"Part A — {name}: MAE = {mae:.3f}, RMSE = {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Part B evaluation ======\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "val_dataset_B = ShanghaiTechDataset(\"../data/ShanghaiTech\", part=\"part_B\", split=\"test_data\", transform=transform)\n",
    "val_loader_B = DataLoader(val_dataset_B, batch_size=1)\n",
    "\n",
    "for name, Cls in [(\"resnet50\", ResNet50Backbone),\n",
    "                  (\"vgg19_bn\", VGG19BNBackbone)]:\n",
    "    model = Cls().to(device)\n",
    "    model.load_state_dict(torch.load(f\"../models/part_B_{name}_model.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    mae = mse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, count_map in val_loader_B:\n",
    "            img = img.to(device)\n",
    "            count = count_map.sum(dim=(1,2)).unsqueeze(1).float().to(device)\n",
    "            out = model(img)\n",
    "            mae += torch.abs(out - count).item()\n",
    "            mse += ((out - count)**2).item()\n",
    "\n",
    "    mae /= len(val_dataset_B)\n",
    "    rmse = (mse / len(val_dataset_B))**0.5\n",
    "    print(f\"Part B — {name}: MAE = {mae:.3f}, RMSE = {rmse:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
