{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb8c93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/vittorio/Projects/uni/CV/CV-crowd-flow-estimation-\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Add project root to sys.path (one directory up from the notebook)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from models.resnet50_backbone import ResNet50Backbone  # Import your custom model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25307dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model\n",
    "def load_model(model_path):\n",
    "    model = ResNet50Backbone()  # Initialize your custom model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Basic saliency map function\n",
    "def get_saliency_map(model, image_tensor, target_class=None):\n",
    "    # Enable gradients for input\n",
    "    image_tensor.requires_grad_()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(image_tensor)\n",
    "    \n",
    "    # If target class is not specified, use the predicted class\n",
    "    if target_class is None:\n",
    "        if isinstance(outputs, tuple):  # Handle if your model returns multiple outputs\n",
    "            outputs = outputs[0]  # Assuming first output is the class prediction\n",
    "        target_class = outputs.argmax(dim=1).item()\n",
    "    \n",
    "    # Zero gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass for the target class\n",
    "    if isinstance(outputs, tuple):\n",
    "        outputs = outputs[0]\n",
    "    \n",
    "    # Create one-hot encoding for the target class\n",
    "    one_hot = torch.zeros_like(outputs)\n",
    "    one_hot[0, target_class] = 1\n",
    "    \n",
    "    # Backward pass\n",
    "    outputs.backward(gradient=one_hot)\n",
    "    \n",
    "    # Get gradients and take absolute value\n",
    "    gradients = image_tensor.grad.data.abs()\n",
    "    \n",
    "    # Take maximum across color channels\n",
    "    saliency, _ = torch.max(gradients, dim=1)\n",
    "    \n",
    "    return saliency.squeeze().cpu().numpy()\n",
    "\n",
    "# Load and preprocess an image\n",
    "def preprocess_image(image_path, size=(224, 224)):\n",
    "    # Define preprocessing steps (adapt to match your model's preprocessing)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        # Use your model's normalization values\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image, input_tensor\n",
    "\n",
    "# Grad-CAM implementation for ResNet backbone\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        self.hooks = []\n",
    "        self.hooks.append(self.target_layer.register_forward_hook(self._forward_hook))\n",
    "        self.hooks.append(self.target_layer.register_backward_hook(self._backward_hook))\n",
    "    \n",
    "    def _forward_hook(self, module, input, output):\n",
    "        self.activations = output\n",
    "    \n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "    \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        # Check if output is a tuple and get the appropriate tensor\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # Adjust based on your model's output structure\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0, target_class] = 1\n",
    "        output.backward(gradient=one_hot)\n",
    "        \n",
    "        # Calculate weights\n",
    "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
    "        \n",
    "        # Apply weights to activations\n",
    "        cam = torch.sum(weights * self.activations, dim=1).squeeze()\n",
    "        \n",
    "        # Apply ReLU to highlight positive contributions\n",
    "        cam = torch.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        if torch.max(cam) > 0:\n",
    "            cam = cam / torch.max(cam)\n",
    "        \n",
    "        return cam.detach().cpu().numpy()\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "# Main function\n",
    "def visualize_saliency(model_path, image_path):\n",
    "    # Load model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    original_image, input_tensor = preprocess_image(image_path)\n",
    "    \n",
    "    # Generate basic saliency map\n",
    "    saliency_map = get_saliency_map(model, input_tensor.clone())\n",
    "    \n",
    "    # For Grad-CAM, find the appropriate layer\n",
    "    # This depends on your ResNet50Backbone implementation\n",
    "    # For standard ResNet50, it's typically the last convolutional layer\n",
    "    # You might need to adjust this for your custom backbone\n",
    "    target_layer = None\n",
    "    \n",
    "    # Try to find the last convolutional layer in your model\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            target_layer = module\n",
    "    \n",
    "    # Check if we found a target layer\n",
    "    if target_layer is not None:\n",
    "        # Initialize Grad-CAM\n",
    "        grad_cam = GradCAM(model, target_layer)\n",
    "        \n",
    "        # Generate CAM\n",
    "        cam = grad_cam.generate_cam(input_tensor)\n",
    "        \n",
    "        # Resize CAM to match original image dimensions\n",
    "        from PIL import Image\n",
    "        cam_image = Image.fromarray(np.uint8(255 * cam))\n",
    "        cam_image = cam_image.resize((original_image.width, original_image.height), Image.LANCZOS)\n",
    "        cam_resized = np.array(cam_image) / 255.0\n",
    "        \n",
    "        # Create heatmap overlay\n",
    "        heatmap = plt.cm.jet(cam_resized)[:, :, :3]\n",
    "        \n",
    "        # Convert original image to numpy array for blending\n",
    "        original_np = np.array(original_image) / 255.0\n",
    "        \n",
    "        # Overlay heatmap on original image (60% original, 40% heatmap)\n",
    "        superimposed = 0.6 * original_np + 0.4 * heatmap\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(original_image)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(saliency_map, cmap='hot')\n",
    "        plt.title('Vanilla Saliency Map')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(superimposed)\n",
    "        plt.title('Grad-CAM Heatmap')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Clean up\n",
    "        grad_cam.remove_hooks()\n",
    "    else:\n",
    "        # If target layer wasn't found, just show saliency map\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_image)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(saliency_map, cmap='hot')\n",
    "        plt.title('Saliency Map')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3a10a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/part_A_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33m../models/part_A_model.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33m../data/ShanghaiTech/part_A/train_data/images/IMG_5.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mvisualize_saliency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mvisualize_saliency\u001b[39m\u001b[34m(model_path, image_path)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mvisualize_saliency\u001b[39m(model_path, image_path):\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Load and preprocess image\u001b[39;00m\n\u001b[32m    119\u001b[39m     original_image, input_tensor = preprocess_image(image_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mload_model\u001b[39m(model_path):\n\u001b[32m      3\u001b[39m     model = ResNet50Backbone()  \u001b[38;5;66;03m# Initialize your custom model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      5\u001b[39m     model.eval()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../models/part_A_model.pth'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model_path = '../models/part_A_model.pth'\n",
    "image_path = '../data/ShanghaiTech/part_A/train_data/images/IMG_5.jpg'\n",
    "visualize_saliency(model_path, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c1f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
